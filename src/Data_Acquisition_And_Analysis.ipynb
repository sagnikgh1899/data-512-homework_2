{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Considering Bias in Data - Homework 2\n",
    "\n",
    "The goal of this assignment is to explore the concept of bias in data using Wikipedia articles. This assignment will consider articles about cities in different US states. For this assignment, we will combine a dataset of Wikipedia articles with a dataset of state populations, and use a machine learning service called ORES to estimate the quality of the articles about the cities.\n",
    "\n",
    "## Step 1: Getting the Article, Population and Region Data\n",
    "\n",
    "The first step is getting the data, which lives in several different places. You will need data that lists Wikipedia articles about US cities and data for US state populations.The Wikipedia [Category:Lists of cities in the United States by state](https://en.wikipedia.org/wiki/Category:Lists_of_cities_in_the_United_States_by_state) was crawled to generate a list of Wikipedia article pages about US cities from each state. This data is called as 'us_cities_by_state_SEPT.2023.csv' in the notebook.\n",
    "\n",
    "### 1.a. Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time, urllib.parse\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# Suppressing the less important warnings.\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b. Loading the .csv files into dataframes\n",
    "\n",
    "The US Census Bureau provides updated population estimates for every US state. We can find [State Population Totals and Components of Change: 2020-2022](https://www.census.gov/data/tables/time-series/demo/popest/2020s-state-total.html) on their website. An Excel file linked to that page contains estimated populations of all US states for 2022.\n",
    "\n",
    "The 'region' demarcation within the US is not one standardized and fixed thing. In fact, different US government agencies agglomerate states to define regions as a function of differing goals (e.g., see List of regions of the United States for some examples). For this analysis, we will use the regional and divisional agglomerations as defined by the US Census Bureau and mentioned in 'US States by Region - US Census Bureau.csv'.\n",
    "\n",
    "There are 3 input files:\n",
    "1. **us_cities_by_state_SEPT.2023.csv**: List of Wikipedia article pages about US cities from each state.\n",
    "2. **population_by_states_2022.csv**: Contains estimated populations of all US states for 2022.\n",
    "3. **US States by Region - US Census Bureau.csv**: Regional and divisional agglomerations as defined by the US Census Bureau and used for analysis in this notebook.\n",
    "\n",
    "The files are read into 3 dataframes:\n",
    "- Cities by states is saved into a dataframe called **df_cities**\n",
    "- Population by states is saved into a dataframe called **df_pop**\n",
    "- US states based on regions and divisions are saved into a dataframe called **region_df**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cities = pd.read_csv('../input files/us_cities_by_state_SEPT.2023.csv')\n",
    "df_pop = pd.read_csv('../input files/population_by_states_2022.csv')\n",
    "region_df = pd.read_csv('../input files/US States by Region - US Census Bureau.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We create a dictionary called state_to_region_division to map states to their corresponding regions and divisions based on data from the region_df. This mapping is used to add \"Region\" and \"Division\" columns to the df_pop DataFrame.\n",
    "- We define a custom sorting key function called custom_sort_key that assigns a sorting order to each state based on its region and division. This function is used to sort the df_pop DataFrame in a customized order.\n",
    "- We perform various data manipulations, including sorting the DataFrame using the custom sorting key, filtering out rows with 'NaN' Division, and converting the \"Population Estimate\" column to numeric format. \n",
    "- Finally, we group the data by regional divisions and calculate the population sum for each division, storing the results in a DataFrame named df_pop_division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping dictionary for states, regions, and divisions\n",
    "state_to_region_division = {}\n",
    "current_region = None\n",
    "current_division = None\n",
    "\n",
    "for row in region_df.values:\n",
    "    if not pd.isna(row[0]):\n",
    "        current_region = row[0]\n",
    "        current_division = None\n",
    "    if not pd.isna(row[1]):\n",
    "        current_division = row[1]\n",
    "    if not pd.isna(row[2]):\n",
    "        state = row[2]\n",
    "        state_to_region_division[state] = (current_region, current_division)\n",
    "\n",
    "def map_state_to_region(state):\n",
    "    return state_to_region_division.get(state, ('NaN', 'NaN'))\n",
    "# Adding \"Region\" and \"Division\" columns to df_pop\n",
    "df_pop['Region'], df_pop['Division'] = zip(*df_pop['Geographic Area'].map(map_state_to_region))\n",
    "\n",
    "# Creating a custom sorting key function\n",
    "def custom_sort_key(state):\n",
    "    region, division = state_to_region_division.get(state, ('NaN', 'NaN'))\n",
    "    region_order = ['Northeast', 'Midwest', 'South', 'West']\n",
    "    division_order = [\n",
    "        'New England', 'Middle Atlantic',\n",
    "        'East North Central', 'West North Central',\n",
    "        'South Atlantic', 'East South Central', 'West South Central',\n",
    "        'Mountain', 'Pacific'\n",
    "    ]\n",
    "    # Assigning a high value for 'NaN' regions and divisions to place them at the end\n",
    "    region_index = region_order.index(region) if region in region_order else len(region_order)\n",
    "    division_index = division_order.index(division) if division in division_order else len(division_order)\n",
    "    return (region_index, division_index, state)\n",
    "\n",
    "# Sorting the DataFrame by the custom sorting key\n",
    "df_pop['SortKey'] = df_pop['Geographic Area'].map(custom_sort_key)\n",
    "df_pop = df_pop.sort_values(by='SortKey').drop('SortKey', axis=1).reset_index().drop('index', axis=1)\n",
    "\n",
    "# Filtering out rows with 'NaN' Division\n",
    "df_pop_division = df_pop[df_pop['Division'] != 'NaN']\n",
    "# Removing commas and converting \"Population Estimate\" to numeric\n",
    "df_pop_division['Population Estimate'] = df_pop_division['Population Estimate'].str.replace(',', '').astype(int)\n",
    "df_pop_division = df_pop_division.groupby('Division')['Population Estimate'].sum().reset_index()\n",
    "df_pop_division.columns = ['regional_division', 'population']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the df_pop_division dataframe later on in the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>page_title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abbeville,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Adamsville, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Adamsville,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Addison, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Addison,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Akron, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Akron,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Alabaster, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Alabaster,_Alabama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     state           page_title  \\\n",
       "0  Alabama   Abbeville, Alabama   \n",
       "1  Alabama  Adamsville, Alabama   \n",
       "2  Alabama     Addison, Alabama   \n",
       "3  Alabama       Akron, Alabama   \n",
       "4  Alabama   Alabaster, Alabama   \n",
       "\n",
       "                                                 url  \n",
       "0   https://en.wikipedia.org/wiki/Abbeville,_Alabama  \n",
       "1  https://en.wikipedia.org/wiki/Adamsville,_Alabama  \n",
       "2     https://en.wikipedia.org/wiki/Addison,_Alabama  \n",
       "3       https://en.wikipedia.org/wiki/Akron,_Alabama  \n",
       "4   https://en.wikipedia.org/wiki/Alabaster,_Alabama  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the df_cities dataframe\n",
    "df_cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Geographic Area</th>\n",
       "      <th>Population Estimate</th>\n",
       "      <th>Region</th>\n",
       "      <th>Division</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Connecticut</td>\n",
       "      <td>3,626,205</td>\n",
       "      <td>Northeast</td>\n",
       "      <td>New England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maine</td>\n",
       "      <td>1,385,340</td>\n",
       "      <td>Northeast</td>\n",
       "      <td>New England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>6,981,974</td>\n",
       "      <td>Northeast</td>\n",
       "      <td>New England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>1,395,231</td>\n",
       "      <td>Northeast</td>\n",
       "      <td>New England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rhode Island</td>\n",
       "      <td>1,093,734</td>\n",
       "      <td>Northeast</td>\n",
       "      <td>New England</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Geographic Area Population Estimate     Region     Division\n",
       "0     Connecticut           3,626,205  Northeast  New England\n",
       "1           Maine           1,385,340  Northeast  New England\n",
       "2   Massachusetts           6,981,974  Northeast  New England\n",
       "3   New Hampshire           1,395,231  Northeast  New England\n",
       "4    Rhode Island           1,093,734  Northeast  New England"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the df_pop dataframe\n",
    "df_pop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regional_division</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>East North Central</td>\n",
       "      <td>47097779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>East South Central</td>\n",
       "      <td>19578002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Middle Atlantic</td>\n",
       "      <td>41910858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>25514320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New England</td>\n",
       "      <td>15129548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    regional_division  population\n",
       "0  East North Central    47097779\n",
       "1  East South Central    19578002\n",
       "2     Middle Atlantic    41910858\n",
       "3            Mountain    25514320\n",
       "4         New England    15129548"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the df_pop_division dataframe\n",
    "df_pop_division.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c. Data Cleaning\n",
    "\n",
    "**Some Considerations**\n",
    "\n",
    "Crawling Wikipedia categories to identify relevant page subsets can result in misleading and/or duplicate category labels. A data crawl can result in possible duplicate articles linked from differently named sub-categories. Naturally, the data crawl attempts to resolve some of these problems, but not all may have been caught. The below section talks about how we have handled the inconsistencies in the data.\n",
    "\n",
    "#### Checking for duplicates in both the dataframes and removing those records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows for cities dataframe =  22157\n",
      "Number of rows for population dataframe =  52\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows for cities dataframe = \", len(df_cities))\n",
    "print(\"Number of rows for population dataframe = \", len(df_pop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cities = df_cities.drop_duplicates(subset=['state', 'page_title', 'url'], keep = 'last')\n",
    "df_cities = df_cities.reset_index(drop=True)\n",
    "df_pop = df_pop.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows for cities dataframe ater removing the duplicates =  21525\n",
      "Number of rows for population dataframe ater removing the duplicates =  52\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows for cities dataframe ater removing the duplicates = \", len(df_cities))\n",
    "print(\"Number of rows for population dataframe ater removing the duplicates = \", len(df_pop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two rows from the cities dataframe has been deleted and there were no duplicates in the population dataframe\n",
    "\n",
    "#### Checking for data inconsistencies like nulls/zero numeric values\n",
    "\n",
    "##### Checking for NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state         0\n",
       "page_title    0\n",
       "url           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cities.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Geographic Area        0\n",
       "Population Estimate    0\n",
       "Region                 0\n",
       "Division               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pop.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no NULL values in df_cities and df_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking for ZERO values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Geographic Area</th>\n",
       "      <th>Population Estimate</th>\n",
       "      <th>Region</th>\n",
       "      <th>Division</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Geographic Area, Population Estimate, Region, Division]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pop[df_pop['Population Estimate'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no 0 population values in the df_pop dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Getting Article Quality Predictions\n",
    "\n",
    "Now we need to get the predicted quality scores for each article in the Wikipedia dataset. We're using a machine learning system called [ORES](https://www.mediawiki.org/wiki/ORES). This was originally an acronym for \"Objective Revision Evaluation Service\" but was simply renamed “ORES”. ORES is a machine learning tool that can provide estimates of Wikipedia article quality. The article quality estimates are, from best to worst:\n",
    "1. FA - Featured article\n",
    "2. GA - Good article (sometimes called A-class)\n",
    "3. B - B-class article\n",
    "4. C - C-class article\n",
    "5. Start - Start-class article\n",
    "6. Stub - Stub-class article\n",
    "These labelings were learned based on articles in Wikipedia that were peer-reviewed using the Wikipedia content assessment procedures.These quality classes are a subset of quality assessment categories developed by Wikipedia editors.\n",
    "\n",
    "\n",
    "To get a Wikipedia page quality prediction from ORES for each cities’s article page we will need to:\n",
    "- read each line of us_cities_by_state_SEPT.2023.csv\n",
    "- make a page info request to get the current page revision\n",
    "- make an ORES request using the page title and current revision id.\n",
    "\n",
    "### 2.a. Configuring the API parameters\n",
    "This example illustrates how to access page info data using the [MediaWiki REST API for the EN Wikipedia](https://www.mediawiki.org/wiki/API:Main_page). This example shows how to request summary 'page info' for a single article page. The API documentation, [API:Info](https://www.mediawiki.org/wiki/API:Info), covers additional details that may be helpful when trying to use or understand this example.\n",
    "\n",
    "**License**\n",
    "\n",
    "This code example was developed by Dr. David W. McDonald for use in DATA 512, a course in the UW MS Data Science degree program. This code is provided under the [Creative Commons](https://creativecommons.org) [CC-BY license](https://creativecommons.org/licenses/by/4.0/). Revision 1.1 - August 14, 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The basic English Wikipedia API endpoint\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# We'll assume that there needs to be some throttling for these requests - we should always be nice to a free data resource\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0) - API_LATENCY_ASSUMED\n",
    "\n",
    "# When making automated requests we should include something that is unique to the person making the request\n",
    "# This should include an email - your UW email would be good to put in there\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<sagnik99@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2023',\n",
    "}\n",
    "\n",
    "# This is a list of politicians from Wikipedia article titles \n",
    "ARTICLE_TITLES = df_cities['page_title']\n",
    "\n",
    "# This is a string of additional page properties that can be returned see the Info documentation for\n",
    "# what can be included. If you don't want any this can simply be the empty string\n",
    "PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers\"\n",
    "#PAGEINFO_EXTENDED_PROPERTIES = \"\"\n",
    "\n",
    "# This template lists the basic parameters for making this\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",           # to simplify this should be a single page title at a time\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b. API request function\n",
    "\n",
    "The API request will be made using one procedure. The idea is to make this reusable. The procedure is parameterized, but relies on the constants above for the important parameters. The underlying assumption is that this will be used to request data for a set of article pages. Therefore the parameter most likely to change is the article_title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_pageinfo_per_article(article_title = None, \n",
    "                                 endpoint_url = API_ENWIKIPEDIA_ENDPOINT, \n",
    "                                 request_template = PAGEINFO_PARAMS_TEMPLATE,\n",
    "                                 headers = REQUEST_HEADERS):\n",
    "    # article title can be as a parameter to the call or in the request_template\n",
    "    if article_title:\n",
    "        request_template['titles'] = article_title\n",
    "\n",
    "    if not request_template['titles']:\n",
    "        raise Exception(\"Must supply an article title to make a pageinfo request.\")\n",
    "        \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or any other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Iterating through the ARTICLE_TITLES to call the above defined function such that we can get the JSON response from the endpoint.\n",
    "- capturing only the article title and the revision_id for the above API call into a dataframe called **df_articles_lastrevid**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The article pages not found are to be captured in the error log file named as \"API_request_error_log.txt\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageinfo_list = {}\n",
    "with open('../intermediate files/API_request_error_log.txt', 'w') as f:\n",
    "\n",
    "    for i in tqdm(range(0, len(ARTICLE_TITLES))):\n",
    "        try: \n",
    "            request_op = request_pageinfo_per_article(article_title = ARTICLE_TITLES[i],\n",
    "                                                      request_template = PAGEINFO_PARAMS_TEMPLATE)['query']['pages']\n",
    "            pageinfo_list.update(request_op)\n",
    "        except:\n",
    "            txt = txt = \"Couldn't get the page info for: \" + i\n",
    "            f.write(txt)\n",
    "            f.write('\\n')\n",
    "    \n",
    "df_articles_lastrevid = pd.DataFrame.from_dict(pageinfo_list, orient='index', columns=['page_title', 'lastrevid'])\n",
    "df_articles_lastrevid.reset_index(inplace = True, drop = True)\n",
    "df_articles_lastrevid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the API call response into a csv file to avoid reloading it multiple times\n",
    "df_articles_lastrevid.to_csv('../intermediate files/request_pageinfo_per_article_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.c. Page information from endpoint\n",
    "\n",
    "This example illustrates how to generate quality scores for article revisions using [ORES](https://www.mediawiki.org/wiki/ORES). This example shows how to request a score of a specific revision, where the score provides probabilities for all of the possible article quality levels. The API documentation can be access from the [ORES API documentation](https://ores.wikimedia.org) page. However, this documentation is a little skimpy and if you want more information you may have to dig around.\n",
    "\n",
    "**License**\n",
    "\n",
    "This code example was developed by Dr. David W. McDonald for use in DATA 512, a course in the UW MS Data Science degree program. This code is provided under the [Creative Commons](https://creativecommons.org) [CC-BY license](https://creativecommons.org/licenses/by/4.0/). Revision 1.0 - August 15, 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The current ORES API endpoint\n",
    "API_ORES_SCORE_ENDPOINT = \"https://ores.wikimedia.org/v3\"\n",
    "# A template for mapping to the URL\n",
    "\n",
    "API_ORES_SCORE_PARAMS = \"/scores/{context}/?models={model}&revids={revids}\"\n",
    "\n",
    "# Use some delays so that we do not hammer the API with our requests\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making automated requests we should include something that is unique to the person making the request\n",
    "# This should include an email - your UW email would be good to put in there\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<sagnik99@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2023'\n",
    "}\n",
    "\n",
    "# This template lists the basic parameters for making an ORES request\n",
    "ORES_PARAMS_TEMPLATE = {\n",
    "    \"context\": \"enwiki\",        # which WMF project for the specified revid\n",
    "    \"revid\" : \"\",               # the revision to be scored - this will probably change each call\n",
    "    \"model\": \"articlequality\"   # the AI/ML scoring model to apply to the reviewion\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_ores_score_per_article(article_revid = None, \n",
    "                                   endpoint_url = API_ORES_SCORE_ENDPOINT, \n",
    "                                   endpoint_params = API_ORES_SCORE_PARAMS, \n",
    "                                   request_template = ORES_PARAMS_TEMPLATE,\n",
    "                                   headers = REQUEST_HEADERS,\n",
    "                                   features=False):\n",
    "    # Make sure we have an article revision id\n",
    "    if not article_revid: \n",
    "        raise Exception(\"Must provide an article revision id (rev_id) to score articles\")\n",
    "    \n",
    "    # set the revision id into the template\n",
    "    request_template['revids'] = article_revid\n",
    "    \n",
    "    # now, create a request URL by combining the endpoint_url with the parameters for the request\n",
    "    request_url = endpoint_url+endpoint_params.format(**request_template)\n",
    "    \n",
    "    # the features used by the ML model can sometimes be returned as well as scores\n",
    "    if features:\n",
    "        request_url = request_url+\"?features=true\"\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like ORES - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the ORES score by running a for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ores_score = {}\n",
    "for i in tqdm(range(0, len(df_articles_lastrevid.lastrevid))):\n",
    "    try:\n",
    "        revids = str(int(df_articles_lastrevid['lastrevid'][i]))\n",
    "        req_op = request_ores_score_per_article(revids)['enwiki']['scores']\n",
    "        ores_score[revids] = req_op[revids]['articlequality']['score']['prediction']\n",
    "    except:\n",
    "        print(\"Couldn't get the ORES info for: \", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ones for which the ORES score wasn't captured is printed below (article_title, lastrevid):\n",
    "1. **Kennebunk, Maine 1172898961**\n",
    "2. **Fraser, Michigan 1162379459**\n",
    "3. **Wildwood Crest, New Jersey 1179887888**\n",
    "    \n",
    "Since, for only three articles, we are not getting the ORES score, we are listing these on the notebook instead of storing in a separate file. Please note that the absence of ORES scores for these three articles may be due to various reasons, such as missing data in the source, temporary unavailability of ORES data, or specific issues related to these articles.\n",
    "\n",
    "Creating a dataframe of the ORES output and renaming the columns for easier tracking and combining later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame.from_dict(ores_score, orient='index', columns=['prediction'])\n",
    "df_scores.reset_index(inplace = True)\n",
    "df_scores = df_scores.rename(columns = {'index': 'lastrevid'})\n",
    "df_scores['lastrevid'] = df_scores['lastrevid'].astype('int')\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the ORES score output dataframe as a .csv file to avoid re-running the code again to retrieve the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.to_csv('../intermediate files/request_ores_score_per_article_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Combining the Datasets\n",
    "\n",
    "Some processing of the data will be necessary. In particular, we'll need to - after retrieving and including the ORES data for each article - merge the Wikipedia data and population data together. Both files have fields containing state names for just that purpose.\n",
    "\n",
    "The combined dataset also requires labeling each state with its US Census regional-division. The [spreadsheet listing the states in each regional division](https://docs.google.com/spreadsheets/d/14Sjfd_u_7N9SSyQ7bmxfebF_2XpR8QamvmNntKDIQB0/edit?usp=sharing) represents the regions, divisions, and states hierarchically. We will need to read this data file and merge it into the resulting dataset.\n",
    "\n",
    "When merging the data, we found entries that cannot be trivially merged. Most likely, the Census Bureau population data includes areas that are not technically states (e.g., \"Washington, D.C., or Puerto Rico, etc). Non-states are ignored. Also, all areas for which there are no matches are identified, and we output a list naming those areas, with each area on a separate line called: **wp_states-no_match.txt**.\n",
    "\n",
    "Finally, we consolidate the merged data into a single CSV file called: **wp_scored_city_articles_by_state.csv**\n",
    "\n",
    "The schema for that file should look something like this:\n",
    "\n",
    "| Column           |\n",
    "|:-----------------|\n",
    "| state            |\n",
    "| regional_division |\n",
    "| population        |\n",
    "| article_title     |\n",
    "| revision_id       |\n",
    "| article_quality   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a. Combining Datasets\n",
    "\n",
    "- cities data (df_cities) has columns: state, page_title, url\n",
    "- page info (df_articles_lastrevid) has columns: (state, lastrevid)\n",
    "- ores score (df_scores) has columns: lastrevid, prediction\n",
    "- population data (df_pop) has columns: Geographic Area, Population Estimate, Region, Division\n",
    "\n",
    "Merging the article page info dataframe with the ORED score prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in Article page info =  21519\n",
      "Number of records in ORES score prediction =  21516\n",
      "Number of records in the joined dataframe with cities and their ORES score prediction = 21525\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>page_title</th>\n",
       "      <th>lastrevid</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>1171163550</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Adamsville, Alabama</td>\n",
       "      <td>1177621427</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Addison, Alabama</td>\n",
       "      <td>1168359898</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Akron, Alabama</td>\n",
       "      <td>1165909508</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Alabaster, Alabama</td>\n",
       "      <td>1179139816</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     state           page_title   lastrevid prediction\n",
       "0  Alabama   Abbeville, Alabama  1171163550          C\n",
       "1  Alabama  Adamsville, Alabama  1177621427          C\n",
       "2  Alabama     Addison, Alabama  1168359898          C\n",
       "3  Alabama       Akron, Alabama  1165909508         GA\n",
       "4  Alabama   Alabaster, Alabama  1179139816          C"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Number of records in Article page info = ', len(df_articles_lastrevid))\n",
    "print('Number of records in ORES score prediction = ', len(df_scores))\n",
    "\n",
    "df_joined = df_articles_lastrevid.merge(df_scores, on = ['lastrevid'], how = 'left')\n",
    "\n",
    "# Adding state as well\n",
    "df_joined = df_cities.merge(df_joined, left_on = \"page_title\", right_on = \"title\", how = 'left')\n",
    "\n",
    "# Cleaning the dataframe by removing duplicate name column (i.e., title) and url\n",
    "df_cities_scores = df_joined.drop(['url', 'title'], axis = 1)\n",
    "print('Number of records in the joined dataframe with cities and their ORES score prediction =',\n",
    "      len(df_cities_scores))\n",
    "\n",
    "# To view a snippet of the dataframe\n",
    "df_cities_scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the population data to this dataframe as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in the joined datafarme = 21540\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>page_title</th>\n",
       "      <th>lastrevid</th>\n",
       "      <th>prediction</th>\n",
       "      <th>Geographic Area</th>\n",
       "      <th>Population Estimate</th>\n",
       "      <th>Region</th>\n",
       "      <th>Division</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>1.171164e+09</td>\n",
       "      <td>C</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>5,074,296</td>\n",
       "      <td>South</td>\n",
       "      <td>East South Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Adamsville, Alabama</td>\n",
       "      <td>1.177621e+09</td>\n",
       "      <td>C</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>5,074,296</td>\n",
       "      <td>South</td>\n",
       "      <td>East South Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Addison, Alabama</td>\n",
       "      <td>1.168360e+09</td>\n",
       "      <td>C</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>5,074,296</td>\n",
       "      <td>South</td>\n",
       "      <td>East South Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Akron, Alabama</td>\n",
       "      <td>1.165910e+09</td>\n",
       "      <td>GA</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>5,074,296</td>\n",
       "      <td>South</td>\n",
       "      <td>East South Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Alabaster, Alabama</td>\n",
       "      <td>1.179140e+09</td>\n",
       "      <td>C</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>5,074,296</td>\n",
       "      <td>South</td>\n",
       "      <td>East South Central</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     state           page_title     lastrevid prediction Geographic Area  \\\n",
       "0  Alabama   Abbeville, Alabama  1.171164e+09          C         Alabama   \n",
       "1  Alabama  Adamsville, Alabama  1.177621e+09          C         Alabama   \n",
       "2  Alabama     Addison, Alabama  1.168360e+09          C         Alabama   \n",
       "3  Alabama       Akron, Alabama  1.165910e+09         GA         Alabama   \n",
       "4  Alabama   Alabaster, Alabama  1.179140e+09          C         Alabama   \n",
       "\n",
       "  Population Estimate Region            Division  \n",
       "0           5,074,296  South  East South Central  \n",
       "1           5,074,296  South  East South Central  \n",
       "2           5,074,296  South  East South Central  \n",
       "3           5,074,296  South  East South Central  \n",
       "4           5,074,296  South  East South Central  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_consolidated = df_cities_scores.merge(df_pop, left_on = 'state', right_on = 'Geographic Area', how = 'outer')\n",
    "print('Number of records in the joined datafarme =', len(df_consolidated))\n",
    "df_consolidated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b. Finding states with no matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Connecticut',\n",
       " 'District of Columbia',\n",
       " 'Georgia',\n",
       " 'Georgia_(U.S._state)',\n",
       " 'Nebraska',\n",
       " 'New Hampshire',\n",
       " 'New Jersey',\n",
       " 'New Mexico',\n",
       " 'New York',\n",
       " 'New_Hampshire',\n",
       " 'New_Jersey',\n",
       " 'New_Mexico',\n",
       " 'New_York',\n",
       " 'North Carolina',\n",
       " 'North Dakota',\n",
       " 'North_Carolina',\n",
       " 'North_Dakota',\n",
       " 'Puerto Rico',\n",
       " 'Rhode Island',\n",
       " 'Rhode_Island',\n",
       " 'South Carolina',\n",
       " 'South Dakota',\n",
       " 'South_Carolina',\n",
       " 'South_Dakota',\n",
       " 'West Virginia',\n",
       " 'West_Virginia']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# s1 is a list for checking for states with no wiki data\n",
    "# creating sets and taking a set difference for the no matches count\n",
    "\n",
    "s1 = df_consolidated[df_consolidated['state'].isnull()]['Geographic Area'].unique()\n",
    "\n",
    "# s2 is a list for checking states with no population data\n",
    "s2 = df_consolidated[df_consolidated['Geographic Area'].isnull()]['state'].unique()\n",
    "\n",
    "no_match = list(set(np.append(s1, s2)))\n",
    "no_match.sort()\n",
    "no_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing to an output text file no_match.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('../output files/wp_areas-no_match.txt', 'w') as f:\n",
    "    for i in no_match:\n",
    "        f.write(i)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consolidate the remaining data into a single CSV file\n",
    "\n",
    "Checking for nulls in geographic area & state, if yes then we drop those columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consolidated = df_consolidated[(~df_consolidated['state'].isnull()) & (~df_consolidated['Geographic Area'].isnull())]\n",
    "df_consolidated = df_consolidated.drop('Geographic Area', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming all the columns as per the standard given in the instruction file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consolidated = df_consolidated.rename(columns = {\n",
    "    'state' : 'state',\n",
    "    'Division' : 'regional_division',\n",
    "    'Population Estimate': 'population',\n",
    "    'page_title' : 'article_title',\n",
    "    'lastrevid' : 'revision_id',\n",
    "    'prediction': 'article_quality'\n",
    "}).drop('Region', axis=1)\n",
    "desired_order = ['state', 'regional_division', 'population', 'article_title', 'revision_id', 'article_quality']\n",
    "df_consolidated = df_consolidated[desired_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving df_consolidated into a csv file as required and viewing the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>regional_division</th>\n",
       "      <th>population</th>\n",
       "      <th>article_title</th>\n",
       "      <th>revision_id</th>\n",
       "      <th>article_quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>5,074,296</td>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>1.171164e+09</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>5,074,296</td>\n",
       "      <td>Adamsville, Alabama</td>\n",
       "      <td>1.177621e+09</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>5,074,296</td>\n",
       "      <td>Addison, Alabama</td>\n",
       "      <td>1.168360e+09</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>5,074,296</td>\n",
       "      <td>Akron, Alabama</td>\n",
       "      <td>1.165910e+09</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>5,074,296</td>\n",
       "      <td>Alabaster, Alabama</td>\n",
       "      <td>1.179140e+09</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     state   regional_division population        article_title   revision_id  \\\n",
       "0  Alabama  East South Central  5,074,296   Abbeville, Alabama  1.171164e+09   \n",
       "1  Alabama  East South Central  5,074,296  Adamsville, Alabama  1.177621e+09   \n",
       "2  Alabama  East South Central  5,074,296     Addison, Alabama  1.168360e+09   \n",
       "3  Alabama  East South Central  5,074,296       Akron, Alabama  1.165910e+09   \n",
       "4  Alabama  East South Central  5,074,296   Alabaster, Alabama  1.179140e+09   \n",
       "\n",
       "  article_quality  \n",
       "0               C  \n",
       "1               C  \n",
       "2               C  \n",
       "3              GA  \n",
       "4               C  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_consolidated.to_csv('../output files/wp_scored_city_articles_by_state.csv', index=False)\n",
    "df_consolidated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Analysis\n",
    "\n",
    "Our analysis will consist of calculating total-articles-per-population (a ratio representing the number of articles per person) and high-quality-articles-per-population (a ratio representing the number of high-quality articles per person) on a state-by-state and divisional basis. All of these values are \"per capita\" ratios. For this analysis, we should consider \"high-quality\" articles to be articles that ORES predicted would be in either the \"FA\" (featured article) or \"GA\" (good article) classes.\n",
    "\n",
    "### 4.a. Total Articles per Population (articles per capita)\n",
    "\n",
    "#### By State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On a state level, the dataframe returns the below number of rows\n",
      "37\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>population</th>\n",
       "      <th>article_count</th>\n",
       "      <th>articles_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>5074296</td>\n",
       "      <td>461</td>\n",
       "      <td>0.000091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>733583</td>\n",
       "      <td>149</td>\n",
       "      <td>0.000203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>7359197</td>\n",
       "      <td>91</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>3045637</td>\n",
       "      <td>500</td>\n",
       "      <td>0.000164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>39029342</td>\n",
       "      <td>482</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        state  population  article_count  articles_per_capita\n",
       "0     Alabama     5074296            461             0.000091\n",
       "1      Alaska      733583            149             0.000203\n",
       "2     Arizona     7359197             91             0.000012\n",
       "3    Arkansas     3045637            500             0.000164\n",
       "4  California    39029342            482             0.000012"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing the duplicates for states, \n",
    "# group the states and aggregate population per regional division by counting number of articles, \n",
    "# calculate article_per_capita\n",
    "df1 = df_consolidated[~df_consolidated.duplicated(subset=['state', 'regional_division'], keep = 'last')]\n",
    "\n",
    "# Calculating the population of each state\n",
    "state_pop = df1[['state', 'population']].groupby('state').sum().reset_index()\n",
    "state_article_cnt = df_consolidated[['state', 'article_title']].groupby('state').count().reset_index()\n",
    "total_articles_state = state_pop.merge(state_article_cnt, on='state')\n",
    "total_articles_state.columns=['state', 'population', 'article_count']\n",
    "total_articles_state['article_count'] = total_articles_state['article_count'].astype('int')\n",
    "total_articles_state['population'] = total_articles_state['population'].str.replace(',', '').astype('int')\n",
    "total_articles_state['articles_per_capita'] = total_articles_state['article_count'] / (total_articles_state['population'])\n",
    "total_articles_state['articles_per_capita'] = total_articles_state['articles_per_capita'].astype('float')\n",
    "\n",
    "# handling for conditions where population is zero (0 states) - still keeping this check\n",
    "total_articles_state = total_articles_state[total_articles_state['articles_per_capita'] != np.inf] \n",
    "print('On a state level, the dataframe returns the below number of rows')\n",
    "print(len(total_articles_state['state'].unique()))\n",
    "total_articles_state.reset_index(inplace=True)\n",
    "total_articles_state = total_articles_state.drop('index', axis = 1)\n",
    "total_articles_state.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By Regional Division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On a regional division level, the dataframe returns the below number of rows\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regional_division</th>\n",
       "      <th>population</th>\n",
       "      <th>article_count</th>\n",
       "      <th>articles_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>East North Central</td>\n",
       "      <td>47097779</td>\n",
       "      <td>4754</td>\n",
       "      <td>0.000101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>East South Central</td>\n",
       "      <td>19578002</td>\n",
       "      <td>1529</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Middle Atlantic</td>\n",
       "      <td>41910858</td>\n",
       "      <td>2556</td>\n",
       "      <td>0.000061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>25514320</td>\n",
       "      <td>1083</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New England</td>\n",
       "      <td>15129548</td>\n",
       "      <td>1164</td>\n",
       "      <td>0.000077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    regional_division  population  article_count  articles_per_capita\n",
       "0  East North Central    47097779           4754             0.000101\n",
       "1  East South Central    19578002           1529             0.000078\n",
       "2     Middle Atlantic    41910858           2556             0.000061\n",
       "3            Mountain    25514320           1083             0.000042\n",
       "4         New England    15129548           1164             0.000077"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repeating the same as above but grouping by regional division in this case\n",
    "# Calculating the population of each state\n",
    "\n",
    "division_pop = df_pop_division # Using from step 1\n",
    "division_article_cnt = df_consolidated[['regional_division', 'article_title']].groupby('regional_division').count().reset_index()\n",
    "total_articles_division = division_pop.merge(division_article_cnt, on='regional_division')\n",
    "total_articles_division.columns=['regional_division', 'population', 'article_count']\n",
    "total_articles_division['articles_per_capita'] = total_articles_division['article_count'] / (total_articles_division['population'])\n",
    " \n",
    "print('On a regional division level, the dataframe returns the below number of rows')\n",
    "print(len(total_articles_division['regional_division'].unique()))\n",
    "total_articles_division.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.b. High Quality Articles per Population\n",
    "\n",
    "This applies to only the articles tagged with FA or GA in the \"article_quality\" column\n",
    "\n",
    "#### By State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On a state level, the high quality dataframe returns the below number of rows\n",
      "37\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>population</th>\n",
       "      <th>article_count</th>\n",
       "      <th>articles_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>5074296</td>\n",
       "      <td>53</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>733583</td>\n",
       "      <td>31</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>7359197</td>\n",
       "      <td>24</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>3045637</td>\n",
       "      <td>72</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>39029342</td>\n",
       "      <td>173</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        state  population  article_count  articles_per_capita\n",
       "0     Alabama     5074296             53             0.000010\n",
       "1      Alaska      733583             31             0.000042\n",
       "2     Arizona     7359197             24             0.000003\n",
       "3    Arkansas     3045637             72             0.000024\n",
       "4  California    39029342            173             0.000004"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering the article based on the artcile_quality attribute\n",
    "# Calculation for article_count and article_per_capita done the same as above i.e., group by state\n",
    "\n",
    "df3 = df_consolidated[~df_consolidated.duplicated(subset=['state', 'regional_division'], keep = 'last')]\n",
    "\n",
    "state_pop = df3[['state', 'population']].groupby('state').sum().reset_index()\n",
    "hq_state_df = df_consolidated[(df_consolidated['article_quality'] == \n",
    "                                 'FA') | (df_consolidated['article_quality'] == 'GA')]\n",
    "\n",
    "state_count = hq_state_df[['state', 'article_title']].groupby('state').count().reset_index()\n",
    "hq_state_df = state_pop.merge(state_count, on='state')\n",
    "hq_state_df.columns=['state', 'population', 'article_count']\n",
    "hq_state_df['article_count'] = hq_state_df['article_count'].astype('int')\n",
    "hq_state_df['population'] = hq_state_df['population'].str.replace(',', '').astype('int')\n",
    "hq_state_df['articles_per_capita'] = hq_state_df['article_count'] / (hq_state_df['population'])\n",
    "hq_state_df['articles_per_capita'] = hq_state_df['articles_per_capita'].astype('float')\n",
    "\n",
    "# Need to exclude conditions where the population of a state is zero\n",
    "hq_state_df = hq_state_df[hq_state_df['articles_per_capita'] != np.inf]\n",
    "hq_state_df.reset_index(inplace=True)\n",
    "hq_state_df.drop(columns=['index'], inplace=True)\n",
    "\n",
    "print('On a state level, the high quality dataframe returns the below number of rows')\n",
    "print(len(hq_state_df['state'].unique()))\n",
    "hq_state_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By Regional Division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On a regional division level, the high quality dataframe returns the below number of rows\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regional_division</th>\n",
       "      <th>population</th>\n",
       "      <th>article_count</th>\n",
       "      <th>articles_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>East North Central</td>\n",
       "      <td>47097779</td>\n",
       "      <td>716</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>East South Central</td>\n",
       "      <td>19578002</td>\n",
       "      <td>316</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Middle Atlantic</td>\n",
       "      <td>41910858</td>\n",
       "      <td>564</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>25514320</td>\n",
       "      <td>304</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New England</td>\n",
       "      <td>15129548</td>\n",
       "      <td>150</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    regional_division  population  article_count  articles_per_capita\n",
       "0  East North Central    47097779            716             0.000015\n",
       "1  East South Central    19578002            316             0.000016\n",
       "2     Middle Atlantic    41910858            564             0.000013\n",
       "3            Mountain    25514320            304             0.000012\n",
       "4         New England    15129548            150             0.000010"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering the article based on the artcile_quality attribute\n",
    "# Calculation for article_count and article_per_capita done the same as above i.e., group by regional division\n",
    "\n",
    "division_pop = df_pop_division #Using from step 1\n",
    "\n",
    "hq_division_df = df_consolidated[(df_consolidated['article_quality'] == \n",
    "                                 'FA') | (df_consolidated['article_quality'] == 'GA')]\n",
    "division_count = hq_division_df[['regional_division', 'article_title']].groupby('regional_division').count().reset_index()\n",
    "hq_division_df = division_pop.merge(division_count, on='regional_division')\n",
    "hq_division_df.columns=['regional_division', 'population', 'article_count']\n",
    "hq_division_df['articles_per_capita'] = hq_division_df['article_count'] / (hq_division_df['population'])\n",
    "\n",
    "print('On a regional division level, the high quality dataframe returns the below number of rows')\n",
    "print(len(hq_division_df['regional_division'].unique()))\n",
    "hq_division_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Results\n",
    "\n",
    "The results from our analysis will be produced in the form of data tables. We are being asked to produce six total tables as follows:\n",
    "\n",
    "### 1. Top 10 US states by coverage: The 10 US states with the highest total articles per capita (in descending order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1          Vermont\n",
       "2            Maine\n",
       "3             Iowa\n",
       "4           Alaska\n",
       "5     Pennsylvania\n",
       "6         Michigan\n",
       "7          Wyoming\n",
       "8         Arkansas\n",
       "9         Missouri\n",
       "10       Minnesota\n",
       "Name: state, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10_state = total_articles_state.sort_values(by=['articles_per_capita'],\n",
    "                                                    ascending=False).head(10).reset_index()\n",
    "top10_state.index += 1\n",
    "top10_state['state']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bottom 10 US states by coverage: The 10 US states with the lowest total articles per capita (in ascending order) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1         Nevada\n",
       "2     California\n",
       "3        Arizona\n",
       "4       Virginia\n",
       "5        Florida\n",
       "6       Oklahoma\n",
       "7         Kansas\n",
       "8       Maryland\n",
       "9      Wisconsin\n",
       "10    Washington\n",
       "Name: state, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottom10_state = total_articles_state.sort_values(by=['articles_per_capita'],\n",
    "                                                    ascending=True).head(10).reset_index()\n",
    "bottom10_state.index += 1\n",
    "bottom10_state['state']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Top 10 US states by high quality: The 10 US states with the highest high quality articles per capita (in descending order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1          Vermont\n",
       "2          Wyoming\n",
       "3          Montana\n",
       "4     Pennsylvania\n",
       "5         Missouri\n",
       "6           Alaska\n",
       "7             Iowa\n",
       "8           Oregon\n",
       "9            Maine\n",
       "10       Minnesota\n",
       "Name: state, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10_hq_state = hq_state_df.sort_values(by=['articles_per_capita'],\n",
    "                                             ascending=False).head(10).reset_index()\n",
    "top10_hq_state.index += 1\n",
    "top10_hq_state['state']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Bottom 10 US states by high quality: The 10 US states with the lowest high quality articles per capita (in ascending order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1          Virginia\n",
       "2            Nevada\n",
       "3           Arizona\n",
       "4        California\n",
       "5           Florida\n",
       "6          Maryland\n",
       "7            Kansas\n",
       "8          Oklahoma\n",
       "9     Massachusetts\n",
       "10        Louisiana\n",
       "Name: state, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottom10_hq_state = hq_state_df.sort_values(by=['articles_per_capita'],\n",
    "                                             ascending=True).head(10).reset_index()\n",
    "bottom10_hq_state.index += 1\n",
    "bottom10_hq_state['state']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Census divisions by total coverage: A rank ordered list of US census divisions (in descending order) by total articles per capita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    West North Central\n",
       "2    East North Central\n",
       "3    East South Central\n",
       "4           New England\n",
       "5       Middle Atlantic\n",
       "6    West South Central\n",
       "7              Mountain\n",
       "8               Pacific\n",
       "9        South Atlantic\n",
       "Name: regional_division, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "division_coverage = total_articles_division.sort_values(by=['articles_per_capita'],\n",
    "                                                ascending=False).reset_index()\n",
    "division_coverage.index += 1\n",
    "division_coverage['regional_division']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Census divisions by high quality coverage: Rank ordered list of US census divisions (in descending order) by high quality articles per capita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    West North Central\n",
       "2    East South Central\n",
       "3    East North Central\n",
       "4    West South Central\n",
       "5       Middle Atlantic\n",
       "6              Mountain\n",
       "7           New England\n",
       "8               Pacific\n",
       "9        South Atlantic\n",
       "Name: regional_division, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "division_hq_coverage = hq_division_df.sort_values(by=['articles_per_capita'],\n",
    "                                           ascending=False).reset_index()\n",
    "division_hq_coverage.index += 1\n",
    "division_hq_coverage['regional_division']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
